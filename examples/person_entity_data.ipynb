{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Matching Example\n",
    "\n",
    "Using FEBRL synthetic data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyent.datasets import generate_febrl_data, remove_nan, sample_xy\n",
    "from pyent.datasets import train_test_validate_stratified_split as ttvs\n",
    "from pyent.features import generate_textual_features\n",
    "from pyent.train import train_txt_baseline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Droping NaN's shape of data is (86506, 23)\n",
      "After Droping NaN's shape of data is (52560, 23)\n"
     ]
    }
   ],
   "source": [
    "master_df = remove_nan(generate_febrl_data(init_seed=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_match    49151\n",
       "match        3409\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.labels.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Development and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = master_df.loc[:, ~master_df.columns.isin([\"labels\"])]\n",
    "y = master_df.loc[:, \"labels\"]\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = ttvs(\n",
    "    features=X, targets=y, test_size=0.1, validate_size=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature set shpae: (36792, 2) and Train target shape 36792\n",
      "Test feature set shpae: (5256, 2) and Test target shape 5256\n",
      "Validation feature set shape: (10512, 2) and Vaiidation target shape 10512\n"
     ]
    }
   ],
   "source": [
    "X_train_txt = generate_textual_features(X_train)\n",
    "X_test_txt = generate_textual_features(X_test)\n",
    "X_val_txt = generate_textual_features(X_val)\n",
    "\n",
    "print(f\"Train feature set shpae: {X_train_txt.shape} and Train target shape {len(y_train)}\\nTest feature set shpae: {X_test_txt.shape} and Test target shape {len(y_test)}\\nValidation feature set shape: {X_val_txt.shape} and Vaiidation target shape {len(y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Transformer based Siamese Neural Network Model as Baseline Model\n",
    "\n",
    "To start, for this model we can just look at the `sentence_l` and `sentence_r` _\"textual\"_ features we generated as shown above.\n",
    "\n",
    "<!-- \n",
    "![example_siamese](../docs/example_siamese.png)\n",
    "<h6>Image Obtained from Quora Blog Post: https://quoraengineering.quora.com/</h6>  \n",
    " -->\n",
    "  \n",
    "1. distill roberta base model fron huggingface\n",
    "2. for negative pairs (i.e. target variabkes with negative class labels) the margin = 0.5\n",
    "3. as distance metric we use cosine distance (1-cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-22 20:36:10 - Load pretrained SentenceTransformer: bert-base-uncased\n",
      "2023-02-22 20:36:11 - No sentence-transformers model found with name /Users/mustafawaheed/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/mustafawaheed/.cache/torch/sentence_transformers/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-22 20:36:12 - Use pytorch device: cpu\n",
      "2023-02-22 20:36:12 - Evaluate model without training\n",
      "2023-02-22 20:36:12 - Binary Accuracy Evaluation of the model on  dataset in epoch 0 after 0 steps:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff088157c8304c83bef770c1a5daf50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-22 20:36:17 - Accuracy with Cosine-Similarity:           96.88\t(Threshold: 0.8663)\n",
      "2023-02-22 20:36:17 - F1 with Cosine-Similarity:                 96.77\t(Threshold: 0.8663)\n",
      "2023-02-22 20:36:17 - Precision with Cosine-Similarity:          100.00\n",
      "2023-02-22 20:36:17 - Recall with Cosine-Similarity:             93.75\n",
      "2023-02-22 20:36:17 - Average Precision with Cosine-Similarity:  99.54\n",
      "\n",
      "2023-02-22 20:36:17 - Accuracy with Manhattan-Distance:           98.44\t(Threshold: 97.6487)\n",
      "2023-02-22 20:36:17 - F1 with Manhattan-Distance:                 98.46\t(Threshold: 100.9428)\n",
      "2023-02-22 20:36:17 - Precision with Manhattan-Distance:          96.97\n",
      "2023-02-22 20:36:17 - Recall with Manhattan-Distance:             100.00\n",
      "2023-02-22 20:36:17 - Average Precision with Manhattan-Distance:  99.91\n",
      "\n",
      "2023-02-22 20:36:17 - Accuracy with Euclidean-Distance:           98.44\t(Threshold: 4.5921)\n",
      "2023-02-22 20:36:17 - F1 with Euclidean-Distance:                 98.46\t(Threshold: 4.5921)\n",
      "2023-02-22 20:36:17 - Precision with Euclidean-Distance:          96.97\n",
      "2023-02-22 20:36:17 - Recall with Euclidean-Distance:             100.00\n",
      "2023-02-22 20:36:17 - Average Precision with Euclidean-Distance:  99.81\n",
      "\n",
      "2023-02-22 20:36:17 - Accuracy with Dot-Product:           89.06\t(Threshold: 56.2552)\n",
      "2023-02-22 20:36:17 - F1 with Dot-Product:                 89.55\t(Threshold: 55.4335)\n",
      "2023-02-22 20:36:17 - Precision with Dot-Product:          85.71\n",
      "2023-02-22 20:36:17 - Recall with Dot-Product:             93.75\n",
      "2023-02-22 20:36:17 - Average Precision with Dot-Product:  92.37\n",
      "\n",
      "2023-02-22 20:36:17 - Start Model Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42a09692f1e4c5f915253e73d3f0605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25efc549a378421bbb266400a86c3031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-22 20:37:02 - Binary Accuracy Evaluation of the model on  dataset after epoch 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a765c29b5ea0490bbab7d09d5276c2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-22 20:37:07 - Accuracy with Cosine-Similarity:           96.88\t(Threshold: 0.8360)\n",
      "2023-02-22 20:37:07 - F1 with Cosine-Similarity:                 96.97\t(Threshold: 0.7954)\n",
      "2023-02-22 20:37:07 - Precision with Cosine-Similarity:          94.12\n",
      "2023-02-22 20:37:07 - Recall with Cosine-Similarity:             100.00\n",
      "2023-02-22 20:37:07 - Average Precision with Cosine-Similarity:  99.63\n",
      "\n",
      "2023-02-22 20:37:07 - Accuracy with Manhattan-Distance:           100.00\t(Threshold: 108.6110)\n",
      "2023-02-22 20:37:07 - F1 with Manhattan-Distance:                 100.00\t(Threshold: 108.6110)\n",
      "2023-02-22 20:37:07 - Precision with Manhattan-Distance:          100.00\n",
      "2023-02-22 20:37:07 - Recall with Manhattan-Distance:             100.00\n",
      "2023-02-22 20:37:07 - Average Precision with Manhattan-Distance:  100.00\n",
      "\n",
      "2023-02-22 20:37:07 - Accuracy with Euclidean-Distance:           100.00\t(Threshold: 4.9154)\n",
      "2023-02-22 20:37:07 - F1 with Euclidean-Distance:                 100.00\t(Threshold: 4.9154)\n",
      "2023-02-22 20:37:07 - Precision with Euclidean-Distance:          100.00\n",
      "2023-02-22 20:37:07 - Recall with Euclidean-Distance:             100.00\n",
      "2023-02-22 20:37:07 - Average Precision with Euclidean-Distance:  100.00\n",
      "\n",
      "2023-02-22 20:37:07 - Accuracy with Dot-Product:           93.75\t(Threshold: 52.8703)\n",
      "2023-02-22 20:37:07 - F1 with Dot-Product:                 93.75\t(Threshold: 52.8703)\n",
      "2023-02-22 20:37:07 - Precision with Dot-Product:          93.75\n",
      "2023-02-22 20:37:07 - Recall with Dot-Product:             93.75\n",
      "2023-02-22 20:37:07 - Average Precision with Dot-Product:  95.79\n",
      "\n",
      "2023-02-22 20:37:07 - Save model to ../output/models/bert-base-uncased-bsz-64-ep-1-2023-02-22_20-36-10\n",
      "2023-02-22 20:37:08 - Evaluate model performance on test set\n",
      "2023-02-22 20:37:08 - Load pretrained SentenceTransformer: ../output/models/bert-base-uncased-bsz-64-ep-1-2023-02-22_20-36-10\n",
      "2023-02-22 20:37:09 - Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9df01961b894dc5a300325b714ce4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-22 20:37:14 - Accuracy with Cosine-Similarity:           96.88\t(Threshold: 0.8013)\n",
      "2023-02-22 20:37:14 - F1 with Cosine-Similarity:                 96.88\t(Threshold: 0.7927)\n",
      "2023-02-22 20:37:14 - Precision with Cosine-Similarity:          96.88\n",
      "2023-02-22 20:37:14 - Recall with Cosine-Similarity:             96.88\n",
      "2023-02-22 20:37:14 - Average Precision with Cosine-Similarity:  98.95\n",
      "\n",
      "2023-02-22 20:37:14 - Accuracy with Manhattan-Distance:           98.44\t(Threshold: 114.4303)\n",
      "2023-02-22 20:37:14 - F1 with Manhattan-Distance:                 98.41\t(Threshold: 114.4303)\n",
      "2023-02-22 20:37:14 - Precision with Manhattan-Distance:          100.00\n",
      "2023-02-22 20:37:14 - Recall with Manhattan-Distance:             96.88\n",
      "2023-02-22 20:37:14 - Average Precision with Manhattan-Distance:  99.00\n",
      "\n",
      "2023-02-22 20:37:14 - Accuracy with Euclidean-Distance:           96.88\t(Threshold: 5.1860)\n",
      "2023-02-22 20:37:14 - F1 with Euclidean-Distance:                 96.88\t(Threshold: 5.2328)\n",
      "2023-02-22 20:37:14 - Precision with Euclidean-Distance:          96.88\n",
      "2023-02-22 20:37:14 - Recall with Euclidean-Distance:             96.88\n",
      "2023-02-22 20:37:14 - Average Precision with Euclidean-Distance:  98.86\n",
      "\n",
      "2023-02-22 20:37:14 - Accuracy with Dot-Product:           90.62\t(Threshold: 52.8439)\n",
      "2023-02-22 20:37:14 - F1 with Dot-Product:                 90.91\t(Threshold: 52.8439)\n",
      "2023-02-22 20:37:14 - Precision with Dot-Product:          88.24\n",
      "2023-02-22 20:37:14 - Recall with Dot-Product:             93.75\n",
      "2023-02-22 20:37:14 - Average Precision with Dot-Product:  96.12\n",
      "\n",
      "2023-02-22 20:37:14 - Test Performance Metrics:\n",
      "{\n",
      "    \"accuracy\": 0.96875,\n",
      "    \"accuracy_threshold\": 0.8012781143188477,\n",
      "    \"f1\": 0.96875,\n",
      "    \"f1_threshold\": 0.7926962971687317,\n",
      "    \"precision\": 0.96875,\n",
      "    \"recall\": 0.96875,\n",
      "    \"ap\": 0.9895125679347826\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "X_train_txt_sample, y_train_sample = sample_xy(X=X_train_txt,y=y_train,num=64)\n",
    "X_test_txt_sample, y_test_sample = sample_xy(X=X_test_txt,y=y_test,num=32)\n",
    "X_val_txt_sample, y_val_sample = sample_xy(X=X_val_txt,y=y_val,num=32)\n",
    "\n",
    "train_txt_baseline(X_train_txt_sample, y_train_sample, X_test_txt_sample, y_test_sample, X_val_txt_sample, y_val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "```bibtex \n",
    "@inproceedings{reimers-2019-sentence-bert,\n",
    "    title     = \"Sentence-BERT: Sentence Embeddings using Siamese   BERT-Networks\",\n",
    "    author    = \"Reimers, Nils and Gurevych, Iryna\",\n",
    "    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
    "    month     = \"11\",\n",
    "    year      = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url       = \"https://arxiv.org/abs/1908.10084\",\n",
    "}\n",
    "```\n",
    "  \n",
    "```bibtex  \n",
    "@software{de_bruin_j_2019_3559043,\n",
    "  author       = \"De Bruin, J\",\n",
    "  title        = \"Python Record Linkage Toolkit: A toolkit for record linkage and duplicate detection in Python\",\n",
    "  month        = \"12\",\n",
    "  year         = \"2019\",\n",
    "  publisher    = \"Zenodo\",\n",
    "  version      = \"v0.14\",\n",
    "  doi          = \"10.5281/zenodo.3559043\",\n",
    "  url          = \"https://doi.org/10.5281/zenodo.3559043\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## df_in is a pandas DataFrame with all the required columns\n",
    "\n",
    "# block_vars = ['area', 'rooms', 'bathrooms', 'garages', 'stratum', 'type']\n",
    "# compare_vars = [\n",
    "#             String('description', 'description', method='lcs',\n",
    "#                    label='description', threshold=0.95),\n",
    "#             Numeric('originPrice', 'originPrice', method='gauss',\n",
    "#                     label='originPrice', offset=0.2, scale=0.2),\n",
    "#             Geographic('latitude', 'longitude', 'latitude', 'longitude',\n",
    "#                        method='gauss', offset=0.2, label='location')\n",
    "#             ]\n",
    "# indexer = rl.index.Block(block_vars)\n",
    "# candidate_links = indexer.index(df_in)\n",
    "# njobs = 8\n",
    "\n",
    "# ## This is the part that takes hours\n",
    "# comparer = rl.Compare(compare_vars, n_jobs=njobs)\n",
    "# compare_vectors = comparer.compute(pairs=candidate_links, x=df_in)\n",
    "\n",
    "# ## Model training doesn't take too long\n",
    "# ecm = rl.ECMClassifier(binarize=0.5)\n",
    "# ecm.fit(compare_vectors)\n",
    "# pairs_ecm = ecm.predict(compare_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from sentence_transformers import losses\n",
    "# from sentence_transformers import LoggingHandler, SentenceTransformer, evaluation\n",
    "# from sentence_transformers.readers import InputExample\n",
    "\n",
    "\n",
    "# ############################################################\n",
    "# logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "#                     datefmt='%Y-%m-%d %H:%M:%S',\n",
    "#                     level=logging.INFO,\n",
    "#                     handlers=[LoggingHandler()])\n",
    "# logger = logging.getLogger(__name__)\n",
    "# ############################################################\n",
    "\n",
    "# # prepare data splits for algorithm\n",
    "# X_train_txt['target'] = np.where(y_train == \"match\", 1, 0)\n",
    "# X_test_txt['target'] = np.where(y_test == \"match\", 1, 0)\n",
    "# X_val_txt['target'] = np.where(y_val == \"match\", 1, 0)\n",
    "\n",
    "\n",
    "# # oaraneters abd configs for training\n",
    "# model_name = 'bert-base-uncased'\n",
    "# num_epochs = 1\n",
    "# train_batch_size = 64\n",
    "# margin = 0.5\n",
    "# model_save_path = '../output/models/{}-bsz-{}-ep-{}-{}'.format(\n",
    "#     model_name, \n",
    "#     train_batch_size,\n",
    "#     num_epochs,\n",
    "#     datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# )\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "# distance_metric = losses.SiameseDistanceMetric.COSINE_DISTANCE\n",
    "# model = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "# # create train and test sample\n",
    "# train_samples = []\n",
    "# for row in  X_train_txt.iterrows():\n",
    "#     if row[1]['target'] == 1:\n",
    "#         train_samples.append(\n",
    "#             InputExample(\n",
    "#                 texts=[\n",
    "#                     row[1]['sentence_l'], \n",
    "#                     row[1]['sentence_r']\n",
    "#                 ], \n",
    "#                 label=int(row[1]['target'])\n",
    "#             )\n",
    "#         )\n",
    "#         train_samples.append(\n",
    "#             InputExample(\n",
    "#                 texts=[\n",
    "#                     row[1]['sentence_r'], \n",
    "#                     row[1]['sentence_l']\n",
    "#                 ], \n",
    "#                 label=int(row[1]['target'])\n",
    "#             )\n",
    "#         )\n",
    "#     else:\n",
    "#         train_samples.append(\n",
    "#             InputExample(\n",
    "#                 texts=[\n",
    "#                     row[1]['sentence_l'], \n",
    "#                     row[1]['sentence_r']\n",
    "#                 ], \n",
    "#                 label=int(row[1]['target'])\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# # initialize data loader and loss definition\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_samples, \n",
    "#     shuffle=True, \n",
    "#     batch_size=train_batch_size\n",
    "# )\n",
    "\n",
    "# train_loss = losses.OnlineContrastiveLoss(\n",
    "#     model=model, \n",
    "#     distance_metric=distance_metric, \n",
    "#     margin=margin\n",
    "# )\n",
    "\n",
    "# evaluators = []\n",
    "\n",
    "# dev_sentences1 = []\n",
    "# dev_sentences2 = []\n",
    "# dev_labels = []\n",
    "# for row in X_val_txt.iterrows():\n",
    "#     dev_sentences1.append(row[1]['sentence_l'])\n",
    "#     dev_sentences2.append(row[1]['sentence_r'])\n",
    "#     dev_labels.append(int(row[1]['target']))\n",
    "\n",
    "# binary_acc_evaluator = evaluation.BinaryClassificationEvaluator(\n",
    "#     sentences1=dev_sentences1, \n",
    "#     sentences2=dev_sentences2, \n",
    "#     labels=dev_labels\n",
    "# )\n",
    "# evaluators.append(binary_acc_evaluator)\n",
    "\n",
    "# # This SequentialEvaluator runs all other evaluators if/when added \n",
    "# seq_evaluator = evaluation.SequentialEvaluator(\n",
    "#     evaluators=evaluators, \n",
    "#     main_score_function=lambda scores: scores[-1]\n",
    "# )\n",
    "\n",
    "# logger.info(\"Evaluate model without training\")\n",
    "# seq_evaluator(\n",
    "#     model=model, \n",
    "#     epoch=0, \n",
    "#     steps=0, \n",
    "#     output_path=model_save_path\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     train_objectives=[(train_dataloader, train_loss)],\n",
    "#     evaluator=seq_evaluator,\n",
    "#     epochs=num_epochs,\n",
    "#     use_amp=True,\n",
    "#     warmup_steps=500,\n",
    "#     output_path=model_save_path,\n",
    "#     show_progress_bar=True\n",
    "# )\n",
    "\n",
    "# bi_encoder = SentenceTransformer(model_save_path)\n",
    "\n",
    "# test_sentence_l = X_test_txt.sentence_l.tolist()\n",
    "# test_sentence_r = X_test_txt.sentence_r.tolist()\n",
    "# test_target = X_test_txt.target.tolist()\n",
    "\n",
    "# test_eval = evaluation.BinaryClassificationEvaluator(\n",
    "#     sentences1=test_sentence_l,\n",
    "#     sentences2=test_sentence_r,\n",
    "#     labels=test_target,\n",
    "#     name=f\"test_evaluator_{os.path.basename(model_save_path)}\",\n",
    "#     batch_size=32,\n",
    "#     write_csv=True,\n",
    "#     show_progress_bar=True\n",
    "# )\n",
    "\n",
    "# test_pref_metrics = test_eval.compute_metrices(bi_encoder)\n",
    "# acc, acc_threshold = test_eval(bi_encoder).find_best_acc_and_threshold()\n",
    "# f1, precision, recall, f1_threshold = test_eval(bi_encoder).find_best_f1_and_threshold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6428ed0887bfe3dd6d1bd9b847bc5095062b29fc6dadf2105fd27a91eb126d90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
