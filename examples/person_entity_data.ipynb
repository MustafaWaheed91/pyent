{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Matching Example\n",
    "\n",
    "Using FEBRL synthetic data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyent.datasets import generate_febrl_data, remove_nan\n",
    "from pyent.datasets import train_test_validate_stratified_split as ttvs\n",
    "from pyent.features import generate_textual_features\n",
    "from pyent.train import train_txt_baseline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = remove_nan(generate_febrl_data(init_seed=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.labels.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Development and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = master_df.loc[:, ~master_df.columns.isin([\"labels\"])]\n",
    "y = master_df.loc[:, \"labels\"]\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = ttvs(\n",
    "    features=X, targets=y, test_size=0.1, validate_size=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_txt = generate_textual_features(X_train)\n",
    "X_test_txt = generate_textual_features(X_test)\n",
    "X_val_txt = generate_textual_features(X_val)\n",
    "\n",
    "print(f\"Train feature set shpae: {X_train_txt.shape} and Train target shape {len(y_train)}\\nTest feature set shpae: {X_test_txt.shape} and Test target shape {len(y_test)}\\nValidation feature set shape: {X_val_txt.shape} and Vaiidation target shape {len(y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Transformer based Siamese Neural Network Model as Baseline Model\n",
    "\n",
    "To start, for this model we can just look at the `sentence_l` and `sentence_r` _\"textual\"_ features we generated as shown above.\n",
    "\n",
    "<!-- \n",
    "![example_siamese](../docs/example_siamese.png)\n",
    "<h6>Image Obtained from Quora Blog Post: https://quoraengineering.quora.com/</h6>  \n",
    " -->\n",
    "  \n",
    "1. distill roberta base model fron huggingface\n",
    "2. for negative pairs (i.e. target variabkes with negative class labels) the margin = 0.5\n",
    "3. as distance metric we use cosine distance (1-cosine_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_baseline(X_train_txt, y_train, X_test_txt, y_test, X_val_txt, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "```bibtex \n",
    "@inproceedings{reimers-2019-sentence-bert,\n",
    "    title     = \"Sentence-BERT: Sentence Embeddings using Siamese   BERT-Networks\",\n",
    "    author    = \"Reimers, Nils and Gurevych, Iryna\",\n",
    "    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
    "    month     = \"11\",\n",
    "    year      = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url       = \"https://arxiv.org/abs/1908.10084\",\n",
    "}\n",
    "```\n",
    "  \n",
    "```bibtex  \n",
    "@software{de_bruin_j_2019_3559043,\n",
    "  author       = \"De Bruin, J\",\n",
    "  title        = \"Python Record Linkage Toolkit: A toolkit for record linkage and duplicate detection in Python\",\n",
    "  month        = \"12\",\n",
    "  year         = \"2019\",\n",
    "  publisher    = \"Zenodo\",\n",
    "  version      = \"v0.14\",\n",
    "  doi          = \"10.5281/zenodo.3559043\",\n",
    "  url          = \"https://doi.org/10.5281/zenodo.3559043\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## df_in is a pandas DataFrame with all the required columns\n",
    "\n",
    "# block_vars = ['area', 'rooms', 'bathrooms', 'garages', 'stratum', 'type']\n",
    "# compare_vars = [\n",
    "#             String('description', 'description', method='lcs',\n",
    "#                    label='description', threshold=0.95),\n",
    "#             Numeric('originPrice', 'originPrice', method='gauss',\n",
    "#                     label='originPrice', offset=0.2, scale=0.2),\n",
    "#             Geographic('latitude', 'longitude', 'latitude', 'longitude',\n",
    "#                        method='gauss', offset=0.2, label='location')\n",
    "#             ]\n",
    "# indexer = rl.index.Block(block_vars)\n",
    "# candidate_links = indexer.index(df_in)\n",
    "# njobs = 8\n",
    "\n",
    "# ## This is the part that takes hours\n",
    "# comparer = rl.Compare(compare_vars, n_jobs=njobs)\n",
    "# compare_vectors = comparer.compute(pairs=candidate_links, x=df_in)\n",
    "\n",
    "# ## Model training doesn't take too long\n",
    "# ecm = rl.ECMClassifier(binarize=0.5)\n",
    "# ecm.fit(compare_vectors)\n",
    "# pairs_ecm = ecm.predict(compare_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from sentence_transformers import losses\n",
    "# from sentence_transformers import LoggingHandler, SentenceTransformer, evaluation\n",
    "# from sentence_transformers.readers import InputExample\n",
    "\n",
    "\n",
    "# ############################################################\n",
    "# logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "#                     datefmt='%Y-%m-%d %H:%M:%S',\n",
    "#                     level=logging.INFO,\n",
    "#                     handlers=[LoggingHandler()])\n",
    "# logger = logging.getLogger(__name__)\n",
    "# ############################################################\n",
    "\n",
    "# # prepare data splits for algorithm\n",
    "# X_train_txt['target'] = np.where(y_train == \"match\", 1, 0)\n",
    "# X_test_txt['target'] = np.where(y_test == \"match\", 1, 0)\n",
    "# X_val_txt['target'] = np.where(y_val == \"match\", 1, 0)\n",
    "\n",
    "\n",
    "# # oaraneters abd configs for training\n",
    "# model_name = 'bert-base-uncased'\n",
    "# num_epochs = 1\n",
    "# train_batch_size = 64\n",
    "# margin = 0.5\n",
    "# model_save_path = '../output/models/{}-bsz-{}-ep-{}-{}'.format(\n",
    "#     model_name, \n",
    "#     train_batch_size,\n",
    "#     num_epochs,\n",
    "#     datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# )\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "# distance_metric = losses.SiameseDistanceMetric.COSINE_DISTANCE\n",
    "# model = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "# # create train and test sample\n",
    "# train_samples = []\n",
    "# for row in  X_train_txt.iterrows():\n",
    "#     if row[1]['target'] == 1:\n",
    "#         train_samples.append(\n",
    "#             InputExample(\n",
    "#                 texts=[\n",
    "#                     row[1]['sentence_l'], \n",
    "#                     row[1]['sentence_r']\n",
    "#                 ], \n",
    "#                 label=int(row[1]['target'])\n",
    "#             )\n",
    "#         )\n",
    "#         train_samples.append(\n",
    "#             InputExample(\n",
    "#                 texts=[\n",
    "#                     row[1]['sentence_r'], \n",
    "#                     row[1]['sentence_l']\n",
    "#                 ], \n",
    "#                 label=int(row[1]['target'])\n",
    "#             )\n",
    "#         )\n",
    "#     else:\n",
    "#         train_samples.append(\n",
    "#             InputExample(\n",
    "#                 texts=[\n",
    "#                     row[1]['sentence_l'], \n",
    "#                     row[1]['sentence_r']\n",
    "#                 ], \n",
    "#                 label=int(row[1]['target'])\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# # initialize data loader and loss definition\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_samples, \n",
    "#     shuffle=True, \n",
    "#     batch_size=train_batch_size\n",
    "# )\n",
    "\n",
    "# train_loss = losses.OnlineContrastiveLoss(\n",
    "#     model=model, \n",
    "#     distance_metric=distance_metric, \n",
    "#     margin=margin\n",
    "# )\n",
    "\n",
    "# evaluators = []\n",
    "\n",
    "# dev_sentences1 = []\n",
    "# dev_sentences2 = []\n",
    "# dev_labels = []\n",
    "# for row in X_val_txt.iterrows():\n",
    "#     dev_sentences1.append(row[1]['sentence_l'])\n",
    "#     dev_sentences2.append(row[1]['sentence_r'])\n",
    "#     dev_labels.append(int(row[1]['target']))\n",
    "\n",
    "# binary_acc_evaluator = evaluation.BinaryClassificationEvaluator(\n",
    "#     sentences1=dev_sentences1, \n",
    "#     sentences2=dev_sentences2, \n",
    "#     labels=dev_labels\n",
    "# )\n",
    "# evaluators.append(binary_acc_evaluator)\n",
    "\n",
    "# # This SequentialEvaluator runs all other evaluators if/when added \n",
    "# seq_evaluator = evaluation.SequentialEvaluator(\n",
    "#     evaluators=evaluators, \n",
    "#     main_score_function=lambda scores: scores[-1]\n",
    "# )\n",
    "\n",
    "# logger.info(\"Evaluate model without training\")\n",
    "# seq_evaluator(\n",
    "#     model=model, \n",
    "#     epoch=0, \n",
    "#     steps=0, \n",
    "#     output_path=model_save_path\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     train_objectives=[(train_dataloader, train_loss)],\n",
    "#     evaluator=seq_evaluator,\n",
    "#     epochs=num_epochs,\n",
    "#     use_amp=True,\n",
    "#     warmup_steps=500,\n",
    "#     output_path=model_save_path,\n",
    "#     show_progress_bar=True\n",
    "# )\n",
    "\n",
    "# bi_encoder = SentenceTransformer(model_save_path)\n",
    "\n",
    "# test_sentence_l = X_test_txt.sentence_l.tolist()\n",
    "# test_sentence_r = X_test_txt.sentence_r.tolist()\n",
    "# test_target = X_test_txt.target.tolist()\n",
    "\n",
    "# test_eval = evaluation.BinaryClassificationEvaluator(\n",
    "#     sentences1=test_sentence_l,\n",
    "#     sentences2=test_sentence_r,\n",
    "#     labels=test_target,\n",
    "#     name=f\"test_evaluator_{os.path.basename(model_save_path)}\",\n",
    "#     batch_size=32,\n",
    "#     write_csv=True,\n",
    "#     show_progress_bar=True\n",
    "# )\n",
    "\n",
    "# test_pref_metrics = test_eval.compute_metrices(bi_encoder)\n",
    "# acc, acc_threshold = test_eval(bi_encoder).find_best_acc_and_threshold()\n",
    "# f1, precision, recall, f1_threshold = test_eval(bi_encoder).find_best_f1_and_threshold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6428ed0887bfe3dd6d1bd9b847bc5095062b29fc6dadf2105fd27a91eb126d90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
