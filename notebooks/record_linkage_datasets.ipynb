{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets --quiet\n",
    "%pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# squad_ds_list = [ ds for ds in datasets.list_datasets() if 'squad' in ds.lower() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
      "\n",
      "The Length of Data is :  89,819,092 records\n",
      "{'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n",
      "Printing Samples from Dataset\n",
      "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515], 'answer_end': [541]} \n",
      "\n",
      "{'text': ['a copper statue of Christ'], 'answer_start': [188], 'answer_end': [213]} \n",
      "\n",
      "{'text': ['the Main Building'], 'answer_start': [279], 'answer_end': [296]} \n",
      "\n",
      "{'text': ['a Marian place of prayer and reflection'], 'answer_start': [381], 'answer_end': [420]} \n",
      "\n",
      "{'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92], 'answer_end': [126]} \n",
      "\n",
      "{'text': ['September 1876'], 'answer_start': [248], 'answer_end': [262]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Example of loading data with streaming = True\n",
    "dataset = datasets.load_dataset('squad', streaming=True)\n",
    "\n",
    "print(list(dataset.keys()))\n",
    "print(dataset['train'].description)\n",
    "print(f\"The Length of Data is : {dataset['train'].dataset_size: ,} records\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "## define the transformation to dataset['train'] here\n",
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda x: {\n",
    "        'id': x['id'],\n",
    "        'answers': { \n",
    "            **x['answers'], \n",
    "            **{\n",
    "                'answer_end': [ x['answers']['answer_start'][0] + \\\n",
    "                               len(x['answers']['text'][0]) ]\n",
    "            }\n",
    "        },\n",
    "        'context': x['context'],\n",
    "        'question': x['question'],\n",
    "        'title': x['title']\n",
    "    }\n",
    ")\n",
    "\n",
    "## Since streaming true then beaware of lazy exec\n",
    "print(\"Printing Samples from Dataset\")\n",
    "for i, sample in enumerate(dataset['train']):\n",
    "    print(sample['answers'], '\\n')\n",
    "    if i > 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/mustafawaheed/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a64344308c4639a97406771157bb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/mustafawaheed/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-cf966adab71fdca4.arrow\n",
      "Loading cached processed dataset at /Users/mustafawaheed/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-040c5c3fef3797ac.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
      "\n",
      "The Length of Data is :  89,819,092 records\n",
      "{'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n",
      "Printing Features from train dataset Dataset: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': {'answer_end': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}\n"
     ]
    }
   ],
   "source": [
    "## Example of same as above when streaming is False\n",
    "dataset = datasets.load_dataset('squad', streaming=False)\n",
    "\n",
    "print(list(dataset.keys()))\n",
    "print(dataset['train'].description)\n",
    "print(f\"The Length of Data is : {dataset['train'].dataset_size: ,} records\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "## define the transformation to dataset['train'] here\n",
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda x: {\n",
    "        'answers': { \n",
    "            **x['answers'], \n",
    "            **{\n",
    "                'answer_end': [ x['answers']['answer_start'][0] + \\\n",
    "                               len(x['answers']['text'][0]) ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Printing Features from train dataset Dataset: {dataset['train'].features}\")\n",
    "\n",
    "# rename column\n",
    "dataset['train'] = dataset['train'].rename_column('title', 'topic') \n",
    "\n",
    "# filter \n",
    "dataset['train'] = dataset['train'].filter(\n",
    "    lambda x: x['topic'] == 'University_of_Notre_Dame' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c826fade4b944f19813fb7f7a1d2c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda x: tokenizer(\n",
    "        x['question'], \n",
    "        x['context'],\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True\n",
    "    ), batched = True, batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
