{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Entity Data\n",
    " \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets --quiet\n",
    "%pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# squad_ds_list = [ ds for ds in datasets.list_datasets() if 'squad' in ds.lower()  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
      "\n",
      "The Length of Data is :  89,819,092 records\n",
      "{'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n",
      "Printing Samples from Dataset\n",
      "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515], 'answer_end': [541]} \n",
      "\n",
      "{'text': ['a copper statue of Christ'], 'answer_start': [188], 'answer_end': [213]} \n",
      "\n",
      "{'text': ['the Main Building'], 'answer_start': [279], 'answer_end': [296]} \n",
      "\n",
      "{'text': ['a Marian place of prayer and reflection'], 'answer_start': [381], 'answer_end': [420]} \n",
      "\n",
      "{'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92], 'answer_end': [126]} \n",
      "\n",
      "{'text': ['September 1876'], 'answer_start': [248], 'answer_end': [262]} \n"
     ]
    }
   ],
   "source": [
    "## Example of loading data with streaming = True\n",
    "dataset = datasets.load_dataset('squad', streaming=True)\n",
    "\n",
    "print(list(dataset.keys()))\n",
    "print(dataset['train'].description)\n",
    "print(f\"The Length of Data is : {dataset['train'].dataset_size: ,} records\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "## define the transformation to dataset['train'] here\n",
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda x: {\n",
    "        'id': x['id'],\n",
    "        'answers': { \n",
    "            **x['answers'], \n",
    "            **{\n",
    "                'answer_end': [ x['answers']['answer_start'][0] + \\\n",
    "                               len(x['answers']['text'][0]) ]\n",
    "            }\n",
    "        },\n",
    "        'context': x['context'],\n",
    "        'question': x['question'],\n",
    "        'title': x['title']\n",
    "    }\n",
    ")\n",
    "\n",
    "## Since streaming true then beaware of lazy exec\n",
    "print(\"Printing Samples from Dataset\")\n",
    "for i, sample in enumerate(dataset['train']):\n",
    "    print(sample['answers'], '\\n')\n",
    "    if i > 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation']\n",
      "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
      "\n",
      "The Length of Data is :  89,819,092 records\n",
      "{'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3c546c7e6b4040b6939a23ff2b5788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Features from train dataset Dataset: {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': {'answer_end': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffe5fa984454fddbfd8d193192459d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Example of same as above when streaming is False\n",
    "dataset = datasets.load_dataset('squad', streaming=False)\n",
    "\n",
    "print(list(dataset.keys()))\n",
    "print(dataset['train'].description)\n",
    "print(f\"The Length of Data is : {dataset['train'].dataset_size: ,} records\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "## define the transformation to dataset['train'] here\n",
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda x: {\n",
    "        'answers': { \n",
    "            **x['answers'], \n",
    "            **{\n",
    "                'answer_end': [ x['answers']['answer_start'][0] + \\\n",
    "                               len(x['answers']['text'][0]) ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Printing Features from train dataset Dataset: {dataset['train'].features}\")\n",
    "\n",
    "# rename column\n",
    "dataset['train'] = dataset['train'].rename_column('title', 'topic') \n",
    "\n",
    "# filter \n",
    "dataset['train'] = dataset['train'].filter(\n",
    "    lambda x: x['topic'] == 'University_of_Notre_Dame' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c826fade4b944f19813fb7f7a1d2c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda x: tokenizer(\n",
    "        x['question'], \n",
    "        x['context'],\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True\n",
    "    ), batched = True, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T11:57:18.344175Z",
     "start_time": "2023-10-29T11:57:10.443502Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyent.datasets import remove_nan, sample_xy, generate_febrl_data, train_test_validate_strat_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T11:57:27.428871Z",
     "start_time": "2023-10-29T11:57:23.446356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropping NaN's shape of data is (86506, 23)\n",
      "After Dropping NaN's shape of data is (52560, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   index       rec_idL         rec_idR given_name_l surname_l street_number_l  \\\n0      2  rec-1866-org    rec-27-dup-0       nathan  campbell              14   \n1      3  rec-2941-org  rec-1744-dup-0         liam     green               2   \n2      5  rec-3748-org  rec-2550-dup-0         iain     noble              84   \n3      6  rec-1595-org  rec-4413-dup-0          gus     white             109   \n4      7  rec-4919-org  rec-2013-dup-0          mia     jolly              15   \n\n           address_1_l      address_2_l       suburb_l postcode_l  ...  \\\n0    la perouse street  st francis room     glengowrie       6148  ...   \n1          benny place         the gums  crescent head       3067  ...   \n2  rischbieth crescent     the big tree   albany creek       6391  ...   \n3        bundey street         ingevale        clayton       6155  ...   \n4       findlay street    hayfield vlge      kincumber       3995  ...   \n\n  surname_r street_number_r           address_1_r  \\\n0  campbell             190  jackie howe crescent   \n1     green              32   sid barnes crescent   \n2     noble              16        torrens street   \n3     white             178        carliles treet   \n4     jolly              25           shout place   \n\n                        address_2_r     suburb_r postcode_r state_r  \\\n0                           bugoren       woorim       6352     nsw   \n1  dudley specialistm edical centre    wingfield       2027     qld   \n2                      old hiloside     rose bay       4510     nsw   \n3                        laureldale  harris park       6180     nsw   \n4                           jinmara    eaglehawk       2229     nsw   \n\n  date_of_birth_r soc_sec_id_r    labels  \n0        19531108      8948230  no_match  \n1        19860328      4528322  no_match  \n2        19050728      6816111  no_match  \n3        19470605      1655664  no_match  \n4        19381031      3608034  no_match  \n\n[5 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>rec_idL</th>\n      <th>rec_idR</th>\n      <th>given_name_l</th>\n      <th>surname_l</th>\n      <th>street_number_l</th>\n      <th>address_1_l</th>\n      <th>address_2_l</th>\n      <th>suburb_l</th>\n      <th>postcode_l</th>\n      <th>...</th>\n      <th>surname_r</th>\n      <th>street_number_r</th>\n      <th>address_1_r</th>\n      <th>address_2_r</th>\n      <th>suburb_r</th>\n      <th>postcode_r</th>\n      <th>state_r</th>\n      <th>date_of_birth_r</th>\n      <th>soc_sec_id_r</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>rec-1866-org</td>\n      <td>rec-27-dup-0</td>\n      <td>nathan</td>\n      <td>campbell</td>\n      <td>14</td>\n      <td>la perouse street</td>\n      <td>st francis room</td>\n      <td>glengowrie</td>\n      <td>6148</td>\n      <td>...</td>\n      <td>campbell</td>\n      <td>190</td>\n      <td>jackie howe crescent</td>\n      <td>bugoren</td>\n      <td>woorim</td>\n      <td>6352</td>\n      <td>nsw</td>\n      <td>19531108</td>\n      <td>8948230</td>\n      <td>no_match</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>rec-2941-org</td>\n      <td>rec-1744-dup-0</td>\n      <td>liam</td>\n      <td>green</td>\n      <td>2</td>\n      <td>benny place</td>\n      <td>the gums</td>\n      <td>crescent head</td>\n      <td>3067</td>\n      <td>...</td>\n      <td>green</td>\n      <td>32</td>\n      <td>sid barnes crescent</td>\n      <td>dudley specialistm edical centre</td>\n      <td>wingfield</td>\n      <td>2027</td>\n      <td>qld</td>\n      <td>19860328</td>\n      <td>4528322</td>\n      <td>no_match</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>rec-3748-org</td>\n      <td>rec-2550-dup-0</td>\n      <td>iain</td>\n      <td>noble</td>\n      <td>84</td>\n      <td>rischbieth crescent</td>\n      <td>the big tree</td>\n      <td>albany creek</td>\n      <td>6391</td>\n      <td>...</td>\n      <td>noble</td>\n      <td>16</td>\n      <td>torrens street</td>\n      <td>old hiloside</td>\n      <td>rose bay</td>\n      <td>4510</td>\n      <td>nsw</td>\n      <td>19050728</td>\n      <td>6816111</td>\n      <td>no_match</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>rec-1595-org</td>\n      <td>rec-4413-dup-0</td>\n      <td>gus</td>\n      <td>white</td>\n      <td>109</td>\n      <td>bundey street</td>\n      <td>ingevale</td>\n      <td>clayton</td>\n      <td>6155</td>\n      <td>...</td>\n      <td>white</td>\n      <td>178</td>\n      <td>carliles treet</td>\n      <td>laureldale</td>\n      <td>harris park</td>\n      <td>6180</td>\n      <td>nsw</td>\n      <td>19470605</td>\n      <td>1655664</td>\n      <td>no_match</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>rec-4919-org</td>\n      <td>rec-2013-dup-0</td>\n      <td>mia</td>\n      <td>jolly</td>\n      <td>15</td>\n      <td>findlay street</td>\n      <td>hayfield vlge</td>\n      <td>kincumber</td>\n      <td>3995</td>\n      <td>...</td>\n      <td>jolly</td>\n      <td>25</td>\n      <td>shout place</td>\n      <td>jinmara</td>\n      <td>eaglehawk</td>\n      <td>2229</td>\n      <td>nsw</td>\n      <td>19381031</td>\n      <td>3608034</td>\n      <td>no_match</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = remove_nan(generate_febrl_data())\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T11:58:22.445912Z",
     "start_time": "2023-10-29T11:58:20.883896Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# noinspection PyTupleAssignmentBalance\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m features_train, features_test, features_validate, targets_train, targets_test, targets_validate \u001B[38;5;241m=\u001B[39m sample_xy(\n\u001B[1;32m      3\u001B[0m     X\u001B[38;5;241m=\u001B[39md\u001B[38;5;241m.\u001B[39mloc[:, \u001B[38;5;28mlist\u001B[39m(d\u001B[38;5;241m.\u001B[39mcolumns[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])], y\u001B[38;5;241m=\u001B[39md\u001B[38;5;241m.\u001B[39mloc[:, d\u001B[38;5;241m.\u001B[39mcolumns[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]], num\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m) \n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 6, got 2)"
     ]
    }
   ],
   "source": [
    "# noinspection PyTupleAssignmentBalance\n",
    "features_train, features_test, features_validate, targets_train, targets_test, targets_validate = sample_xy(\n",
    "    X=d.loc[:, list(d.columns[:-1])], y=d.loc[:, d.columns[-1]], num=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[0;31mSignature:\u001B[0m\n",
       "\u001B[0msample_xy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0mX\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0my\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseries\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0mnum\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;31mDocstring:\u001B[0m\n",
       "sample any df or series by record count and series val\n",
       "\n",
       ":param num: this is an integer that\n",
       ":param y: this is the column representing the target\n",
       ":param X: this is the dataframe of features\n",
       "\n",
       ":returns Tuple[pandas.DataFrame, pandas.DataFrame]: this is the 2 \n",
       "    dataframes of features and targets for training.\n",
       "\u001B[0;31mSource:\u001B[0m   \n",
       "\u001B[0;32mdef\u001B[0m \u001B[0mtrain_test_validate_strat_split\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidate_size\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mfloat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.2\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0;34m\"\"\"\u001B[0m\n",
       "\u001B[0;34m        Split data into dev (i.e. train and validate) and a test set to hold out.\u001B[0m\n",
       "\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m        :param features:\u001B[0m\n",
       "\u001B[0;34m        :param targets:\u001B[0m\n",
       "\u001B[0;34m        :param test_size:\u001B[0m\n",
       "\u001B[0;34m        :param validate_size:\u001B[0m\n",
       "\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m        :return A tuple of dataframes of train test and validate:\u001B[0m\n",
       "\u001B[0;34m        :rtype: Tuple\u001B[0m\n",
       "\u001B[0;34m    \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0;31m# Get test sets\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0mfeatures_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeatures_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mfeatures\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mtargets\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mstratify\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtargets\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mtest_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest_size\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0;31m# Run train_test_split again to get train and validate sets\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0mpost_split_validate_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalidate_size\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0mfeatures_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeatures_validate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets_validate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mfeatures_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mtargets_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mstratify\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtargets_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m        \u001B[0mtest_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpost_split_validate_size\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\n",
       "\u001B[0;34m\u001B[0m    \u001B[0;32mreturn\u001B[0m \u001B[0mfeatures_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeatures_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeatures_validate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets_validate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;31mFile:\u001B[0m      ~/Documents/datascienceprojects/pyent/pyent/datasets.py\n",
       "\u001B[0;31mType:\u001B[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??sample_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
